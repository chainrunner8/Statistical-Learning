{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is explained in Trevor Hastie's book, the logistic regression and LDA models are quite similar in form in the sense that the log-odds is linear in X (parameter vector) in both. The subtelty here is that if our assumption that the parameters (X) are normally distributed holds, then LDA <i>might</i> perform better than logistic regression especially with the smaller training set (```n_train=50```). So ultimately, since we know that we generated normally distributed parameters, we expect LDA to at least match logistic regression if not predict better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statistics import fmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 15\n",
    "\n",
    "def genData(n):\n",
    "\n",
    "    n1 = n2 = n//2\n",
    "    cov_1 = np.diag(np.repeat(1, p)) + 0.2\n",
    "    x_class1 = np.random.multivariate_normal(\n",
    "        mean=np.repeat(3.5, p),\n",
    "        cov=cov_1,\n",
    "        size=n1\n",
    "    )\n",
    "    x_class2 = np.random.multivariate_normal(\n",
    "        mean=np.repeat(2, p),\n",
    "        cov=cov_1,\n",
    "        size=n2\n",
    "    )\n",
    "    y = np.repeat((1, 2), (n1, n2))\n",
    "    data_set = pd.concat([pd.DataFrame(x_class1), pd.DataFrame(x_class2)])\n",
    "    data_set.columns = [f'x_{i+1}' for i in range(p)]\n",
    "    data_set['y'] = y\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLDA(train_set, test_set):\n",
    "\n",
    "    LDA = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "    LDA.fit(\n",
    "        X=train_set[[f'x_{i+1}' for i in range(p)]], \n",
    "        y=train_set['y']\n",
    "    )\n",
    "    y_pred = LDA.predict(X=test_set[[f'x_{i+1}' for i in range(p)]])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def predictLogistic(train_set, test_set):\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    logit.fit(\n",
    "        X=train_set[[f'x_{i+1}' for i in range(p)]],\n",
    "        y=train_set['y']\n",
    "    )\n",
    "    y_pred = logit.predict(X=test_set[[f'x_{i+1}' for i in range(p)]])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyPreds(row):\n",
    "    \n",
    "    if row.y == 2 and row.y_hat == 2:\n",
    "        return 'TP'\n",
    "    elif row.y == 2 and row.y_hat == 1:\n",
    "        return 'FN'\n",
    "    elif row.y == 1 and row.y_hat == 2:\n",
    "        return 'FP'\n",
    "    elif row.y == 1 and row.y_hat == 1:\n",
    "        return 'TN'\n",
    "    \n",
    "    \n",
    "def getAccuracy(df_labels):\n",
    "\n",
    "    pos_negs = df_labels.apply(classifyPreds, axis=1)\n",
    "    cf_matrix = pos_negs.value_counts()\n",
    "    return (cf_matrix['TP'] + cf_matrix['TN']) / sum(cf_matrix)\n",
    "\n",
    "\n",
    "def getBalancedAccuracy(df_labels):\n",
    "\n",
    "    pos_negs = df_labels.apply(classifyPreds, axis=1)\n",
    "    cf_matrix = pos_negs.value_counts()\n",
    "    tpr = cf_matrix['TP'] / (cf_matrix['TP'] + cf_matrix['FN'])\n",
    "    tnr = cf_matrix['TN'] / (cf_matrix['TN'] + cf_matrix['FP'])\n",
    "    return (tpr + tnr) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationAnalysis(n_train):\n",
    "    \n",
    "    classifier_acc = {'LDA': [], 'logit': []}\n",
    "\n",
    "    for _ in range(100):\n",
    "        train = genData(n_train)\n",
    "        test = genData(10_000)\n",
    "\n",
    "        y_hat_LDA = predictLDA(train, test)\n",
    "        y_hat_logit = predictLogistic(train, test)\n",
    "\n",
    "        df_LDA = pd.DataFrame({'y': test['y'], 'y_hat': y_hat_LDA})\n",
    "        df_logit = pd.DataFrame({'y': test['y'], 'y_hat': y_hat_logit})\n",
    "\n",
    "        classifier_acc['LDA'].append(getAccuracy(df_LDA))\n",
    "        classifier_acc['logit'].append(getAccuracy(df_logit))\n",
    "\n",
    "    return pd.DataFrame({'LDA': [round(fmean(classifier_acc['LDA']), 4)], 'logit': [round(fmean(classifier_acc['logit']), 4)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_train=50</th>\n",
       "      <td>0.7564</td>\n",
       "      <td>0.7812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_train=10^4</th>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LDA   logit\n",
       "n_train=50    0.7564  0.7812\n",
       "n_train=10^4  0.8333  0.8333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_50 = classificationAnalysis(50)\n",
    "df_train_10k = classificationAnalysis(10_000)\n",
    "\n",
    "df_results = pd.concat([df_train_50, df_train_10k])\n",
    "df_results.index = pd.Index(['n_train=50', 'n_train=10^4'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an ```n_train=50``` training set, logistic regression clearly outperforms LDA. Since the two class distributions we generated are very close in mean (mu=2 vs mu=3) and have the same covariance matrix, we can hypothesise that in this kind of setting a logit model does a better job at classifying outcomes that overlap strongly when our training set is relatively small, even though the Gaussian assumption is 100% correct. Let's now test that assumption to see if it turns out to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have changed the mean of class 1 from 3 to 3.5, let's now run the experiment again and see how it comes out. We can expect the accuracy numbers to go up since the distributions are now further apart, but what we're more interested in is if there is still a similar gap in accuracy between logit and LDA with the small training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_train=50</th>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_train=10^4</th>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.9266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LDA   logit\n",
       "n_train=50    0.8756  0.9025\n",
       "n_train=10^4  0.9267  0.9266"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_50 = classificationAnalysis(50)\n",
    "df_train_10k = classificationAnalysis(10_000)\n",
    "\n",
    "df_results = pd.concat([df_train_50, df_train_10k])\n",
    "df_results.index = pd.Index(['n_train=50', 'n_train=10^4'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the gap is almost identical (~0.025), which does not seem to favour my assumption that it is due to distribution overlap. Then the gap could perhaps be explained by the way these two different methods optimise the decision boundary. Logistic regression uses maximum likelihood, while LDA calculates the discriminant function with mean and covariance estimates. There's a general saying that logistic regression is more sensitive to outliers than LDA, and that might be the real reason why logit performs better on small data sets: if LDA is more sensitive to outliers then it can lead to a higher variance especially in small data sets, which is not really compensated by any reduction in bias, so ultimately this means lower accuracy. In order to verify that, maybe we could try and calculate the 15-dimensional distance of every point to the decision boundary (for both LDA and logit) and study the variance of that distance, see if it's maybe indeed higher for LDA than for logit (which would basically confirm that LDA is more sensitive to outliers than logit)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
